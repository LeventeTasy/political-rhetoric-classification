{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T21:03:30.851066Z",
     "start_time": "2026-01-07T21:03:30.483268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hu_core_news_lg\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "from tqdm import tqdm"
   ],
   "id": "97250edc71425c0f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-07T21:03:35.209616Z",
     "start_time": "2026-01-07T21:03:30.877368Z"
    }
   },
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"hu_core_news_lg\")\n",
    "except:\n",
    "    print(\"A nagy modell nem található, a közepeset használjuk...\")\n",
    "    nlp = spacy.load(\"hu_core_news_md\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nlp = hu_core_news_lg.load()",
   "id": "3e32913adc3e1eb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "RAW_CSV = r\"../../data/propaganda_articles.csv\"      # A nyers, eredeti fájl\n",
    "PROCESSED_CSV = r\"../../data/preprocessed_data.csv\""
   ],
   "id": "fbce479b320c1352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "csv_file = r\"../../data/propaganda_articles.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8-sig\")"
   ],
   "id": "36bdfed49ad1f6c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.sample()",
   "id": "8cad9ed1d47dfc76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.label.value_counts()",
   "id": "29218d644919771e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "47b13f41f8183ba6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def spacy_preprocess_pipe(texts, batch_size=50):\n",
    "    \"\"\"\n",
    "    Gyorsabb verzió nlp.pipe használatával és tqdm folyamatjelzővel.\n",
    "    \"\"\"\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    # Itt a lényeg: a tqdm-be csomagoljuk az nlp.pipe-ot.\n",
    "    # A 'total=len(texts)' fontos, hogy tudja, mennyi a 100%.\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=batch_size, disable=[\"ner\", \"parser\"]), \n",
    "                    total=len(texts), \n",
    "                    desc=\"Szöveg előkészítése\"):\n",
    "        \n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if token.is_stop or token.is_punct:\n",
    "                continue\n",
    "            if token.like_num:\n",
    "                tokens.append(\"NUM\")\n",
    "            else:\n",
    "                tokens.append(token.lemma_)\n",
    "        \n",
    "        cleaned_texts.append(\" \".join(tokens))\n",
    "    \n",
    "    return cleaned_texts"
   ],
   "id": "3bb74202a055a5d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_training_data():\n",
    "    # 1. Betöltjük a nyers adatokat\n",
    "    if not os.path.exists(RAW_CSV):\n",
    "        raise FileNotFoundError(f\"Nincs meg a forrásfájl: {RAW_CSV}\")\n",
    "        \n",
    "    df_raw = pd.read_csv(RAW_CSV, encoding=\"utf-8-sig\")\n",
    "    df_raw = df_raw.dropna(subset=['title', 'text', 'url'])\n",
    "    \n",
    "    # 2. Megnézzük, van-e már mentett fájl\n",
    "    if os.path.exists(PROCESSED_CSV):\n",
    "        df_processed = pd.read_csv(PROCESSED_CSV, encoding=\"utf-8-sig\")\n",
    "        processed_urls = set(df_processed['url'])\n",
    "        print(f\"Betöltve {len(df_processed)} már feldolgozott cikk.\")\n",
    "    else:\n",
    "        df_processed = pd.DataFrame(columns=['url', 'cleaned_text', 'label'])\n",
    "        processed_urls = set()\n",
    "        print(\"Még nincs feldolgozott fájl.\")\n",
    "\n",
    "    # 3. Kiszűrjük az újakat\n",
    "    df_new = df_raw[~df_raw['url'].isin(processed_urls)].copy()\n",
    "\n",
    "    if df_new.empty:\n",
    "        print(\"Nincs új feldolgozandó cikk! A cache-ből dolgozunk.\")\n",
    "    else:\n",
    "        print(f\"Feldolgozás alatt: {len(df_new)} új cikk...\")\n",
    "        \n",
    "        df_new['full_text'] = df_new['title'] + \": \" + df_new['text']\n",
    "        \n",
    "        # --- ITT HÍVJUK MEG A TQDM-ES FÜGGVÉNYT ---\n",
    "        df_new['cleaned_text'] = spacy_preprocess_pipe(df_new['full_text'].tolist())\n",
    "        \n",
    "        # 4. Mentés\n",
    "        df_to_save = df_new[['url', 'cleaned_text', 'label']]\n",
    "        header_needed = not os.path.exists(PROCESSED_CSV)\n",
    "        \n",
    "        df_to_save.to_csv(PROCESSED_CSV, mode='a', index=False, header=header_needed, encoding='utf-8-sig')\n",
    "        \n",
    "        # Összefűzés a memóriában\n",
    "        df_processed = pd.concat([df_processed, df_to_save], ignore_index=True)\n",
    "        print(\"Új adatok elmentve.\")\n",
    "\n",
    "    return df_processed['cleaned_text'], df_processed['label']"
   ],
   "id": "ae2d201fee15bbc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def spacy_preprocess_one(text):\n",
    "    \"\"\"\n",
    "    Egyetlen string preprocesszálása (inferencia/jóslás idejére).\n",
    "    Ugyanazt a logikát követi, mint a spacy_preprocess_pipe.\n",
    "    \"\"\"\n",
    "    # 1. Itt simán hívjuk az nlp-t, nem pipe-on keresztül\n",
    "    # A disable itt is gyorsít, ha csak lemma kell\n",
    "    doc = nlp(text, disable=[\"ner\", \"parser\"])\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in doc:\n",
    "        # Ugyanaz a szűrés\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        if token.like_num:\n",
    "            clean_tokens.append(\"NUM\")\n",
    "        else:\n",
    "            clean_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(clean_tokens)"
   ],
   "id": "c31122dc42318c4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x, y = get_training_data()\n",
    "y = y.astype(int)\n",
    "x.sample()"
   ],
   "id": "b22f25c48401dde9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y.value_counts()",
   "id": "e2c0e1f6fd6ead62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
